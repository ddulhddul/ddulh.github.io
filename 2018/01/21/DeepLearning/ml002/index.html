<!DOCTYPE html><html prefix="og: http://ogp.me/ns#"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>Chapter3. 신경망 · ddulh's</title><meta name="description" content="잡념"><meta name="og:title" content="Chapter3. 신경망"><meta name="og:type" content="website"><meta name="og:url" content="https://ddulhddul.github.io/2018/01/21/DeepLearning/ml002/"><meta name="og:image" content="http://image.toast.com/aaaaahq/hola_cover.JPG"><meta name="og:description" content="잡념"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/chiangmai.css"><meta name="steem:author" content="@stunstunstun"><meta name="fb:app_id" content="1258629384258634"><link rel="search" type="application/opensearchdescription+xml" href="https://ddulhddul.github.io/atom.xml" title="ddulh's"></head><body class="post"><div id="fb-root"></div><div class="wrap"><header><nav class="navi-post"><a class="navi-post-back" href="javascript:history.back()"><i class="fa fa-arrow-left" aria-hidden="true"></i></a><a class="navi-post-home" href="/"><i class="fa fa-home" aria-hidden="true"></i></a></nav></header><main class="post"><div class="post"><article class="post-block"><h1 class="post-title">Chapter3. 신경망</h1><div class="post-info"><div class="post-info-profile"><a href="https://github.com/stunstunstun" target="_blank"><img src="/image/profile.jpg"></a></div><div class="post-info-details"><div class="post-categories"><a href="/categories/DeepLearning" target="_self"><span>DEEPLEARNING</span></a></div><div class="post-date">Jan 21, 2018</div></div></div><div class="post-share"><div class="fb-like" data-href="https://ddulhddul.github.io/2018/01/21/DeepLearning/ml002/" data-layout="button_count" data-action="like" data-size="small" data-show-faces="true" data-share="false">                 </div><div class="fb-share-button" data-href="https://ddulhddul.github.io/2018/01/21/DeepLearning/ml002/" data-layout="button" data-size="small" data-mobile-iframe="true"></div><div class="fb-follow" data-href="https://www.facebook.com/holaxprogramming/" data-layout="button_count" data-size="small" data-show-faces="true"></div></div><div class="post-content"><p><img src="/images/deeplearning/cover.jpg" alt="밑바닥부터 시작하는 딥러닝"></p>
<ul>
<li>설치 내용<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; python -m pip install --upgrade pip</span><br><span class="line">&gt; pip install six</span><br><span class="line">&gt; pip install python-dateutil</span><br><span class="line">&gt; pip install pyparsing</span><br><span class="line">&gt; pip install numpy-1.14.0+mkl-cp36-cp36m-win32.whl</span><br><span class="line">&gt; pip install matplotlib-2.1.2-cp36-cp36m-win32.whl</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>신경망은 가중치 설정을 수동으로 하던 퍼셉트론에서 벗어나 가중치 매개변수 값을 학습한다.</p>
<hr>
<h1 id="3-1-퍼셉트론에서-신경망으로"><a href="#3-1-퍼셉트론에서-신경망으로" class="headerlink" title="3.1 퍼셉트론에서 신경망으로"></a>3.1 퍼셉트론에서 신경망으로</h1><h2 id="3-1-1-신경망의-예"><a href="#3-1-1-신경망의-예" class="headerlink" title="3.1.1 신경망의 예"></a>3.1.1 신경망의 예</h2><p>퍼셉트론과 다르지 않다.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png" alt="신경망의 예"><br>출처 : <a href="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png" target="_blank" rel="noopener">위키백과</a></p>
<h2 id="3-1-2-퍼셉트론-복습"><a href="#3-1-2-퍼셉트론-복습" class="headerlink" title="3.1.2 퍼셉트론 복습"></a>3.1.2 퍼셉트론 복습</h2><p><img src="/images/deeplearning/perceptronWithBias.PNG" alt="편향을 명시한 퍼셉트론"></p>
<figure class="highlight py"><figcaption><span>식 3.1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = <span class="number">0</span> (b + w1*x1 + w2*x2 &lt;= <span class="number">0</span>)</span><br><span class="line">  = <span class="number">1</span> (b + w1*x1 + w2*x2 &gt; <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><figcaption><span>식 3.2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = h(b + w1*x1 + w2*x2)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><figcaption><span>식 3.3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h(x) = <span class="number">0</span> (x &lt;= <span class="number">0</span>)</span><br><span class="line">     = <span class="number">1</span> (x &gt; <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 <strong>활성화 함수</strong>(h(x)) 라고 한다.</p>
</blockquote>
<h2 id="3-1-3-활성화-함수의-등장"><a href="#3-1-3-활성화-함수의-등장" class="headerlink" title="3.1.3 활성화 함수의 등장"></a>3.1.3 활성화 함수의 등장</h2><p><img src="/images/deeplearning/activationF.PNG" alt="활성화 함수의 처리 과정"></p>
<p>가중치 신호를 조합한 결과가 a라는 노드가 되고, 활성화 함수 h()를 통과하여 y라는 노드로 변환</p>
<blockquote>
<p>일반적으로 단층 퍼셉트론은 단층 네트워크에서 계단함수를 활성화 함수로 사용한 모델. 다층 퍼셉트론은 신경망(여러층 및 시그모이드 함수 등의 매끈한 활성화 함수)을 가리킨다.</p>
</blockquote>
<hr>
<h1 id="3-2-활성화-함수"><a href="#3-2-활성화-함수" class="headerlink" title="3.2 활성화 함수"></a>3.2 활성화 함수</h1><h2 id="3-2-1-시그모이드-함수"><a href="#3-2-1-시그모이드-함수" class="headerlink" title="3.2.1 시그모이드 함수"></a>3.2.1 시그모이드 함수</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h(x) = <span class="number">1</span> / (<span class="number">1</span> + exp(-x))</span><br></pre></td></tr></table></figure>
<ul>
<li>신경망 에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고 다음 뉴런에 전달.</li>
<li>앞장의 퍼셉트론과의 차이점은 활성화 함수 뿐.</li>
</ul>
<h2 id="3-2-2-계단-함수-구현하기"><a href="#3-2-2-계단-함수-구현하기" class="headerlink" title="3.2.2 계단 함수 구현하기"></a>3.2.2 계단 함수 구현하기</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_function</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = x &gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> y.astype(np.int)</span><br></pre></td></tr></table></figure>
<h2 id="3-2-3-계단-함수의-그래프"><a href="#3-2-3-계단-함수의-그래프" class="headerlink" title="3.2.3 계단 함수의 그래프"></a>3.2.3 계단 함수의 그래프</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_function</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.array(x&gt;<span class="number">0</span>, dtype=np.int)</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-5.0</span>, <span class="number">5.0</span>, <span class="number">0.1</span>) <span class="comment"># -5 ~ 5 0.1 단위</span></span><br><span class="line">y = step_function(x)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.ylim(<span class="number">-0.1</span>, <span class="number">1.1</span>) <span class="comment"># y축 범위 지정</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/deeplearning/chapter3.01stairs.PNG" alt="계단함수 그래프"></p>
<h2 id="3-2-4-시그모이드-함수-구현하기"><a href="#3-2-4-시그모이드-함수-구현하기" class="headerlink" title="3.2.4 시그모이드 함수 구현하기"></a>3.2.4 시그모이드 함수 구현하기</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-5.0</span>, <span class="number">5.0</span>, <span class="number">0.1</span>)</span><br><span class="line">y = sigmoid(x)</span><br><span class="line">plt.plot(x,y)</span><br><span class="line">plt.ylim(<span class="number">-0.1</span>, <span class="number">1.1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/deeplearning/chapter3.02sigmoid.PNG" alt="시그모이드함수 그래프"></p>
<h2 id="3-2-5-시그모이드-함수와-계단-함수-비교"><a href="#3-2-5-시그모이드-함수와-계단-함수-비교" class="headerlink" title="3.2.5 시그모이드 함수와 계단 함수 비교"></a>3.2.5 시그모이드 함수와 계단 함수 비교</h2><ul>
<li><p>차이점</p>
<ul>
<li>매끄러움의 차이</li>
<li>계단 함수는 0과 1중 하나의 값을 돌려주는 반면, 시그모이드 함수는 실수를 return</li>
</ul>
</li>
<li><p>공통점</p>
<ul>
<li>입력이 중요하면 큰 값을 출력하고 중요하지 않으면 작은값을 출력</li>
<li>입력이 아무리 작거나 커도 출력은 0과 1사이</li>
<li>비선형 함수</li>
</ul>
</li>
</ul>
<h2 id="3-2-6-비선형-함수"><a href="#3-2-6-비선형-함수" class="headerlink" title="3.2.6 비선형 함수"></a>3.2.6 비선형 함수</h2><ul>
<li>신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다.</li>
<li>신경망의 층을 깊게하는 이유</li>
<li>선형 함수의 문제는 층을 아무리 깊게 해도 ‘은닉층이 없는 네트워크’ 로도 똑같은 기능을 할 수 있다.</li>
</ul>
<h2 id="3-2-7-ReLU-함수"><a href="#3-2-7-ReLU-함수" class="headerlink" title="3.2.7 ReLU 함수"></a>3.2.7 ReLU 함수</h2><p>Rectified Linear Unit (렐루) : 정류된</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h(x) = x (x &gt; <span class="number">0</span>)</span><br><span class="line">     = <span class="number">0</span> (x &lt;= <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.maximun(<span class="number">0</span>,x)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>활성화 함수로 책의 후반부에서 주로 ReLU함수 사용.</p>
</blockquote>
<ul>
<li>Why ?<ul>
<li>계산의 과정이 Simple, 계산을 빠르게 하기 위해. 6배 정도?</li>
<li>음수표현이나 실수표현이나 지수 연산이 없다.</li>
<li>vanishing gradient 를 방지.</li>
</ul>
</li>
</ul>
<hr>
<h1 id="3-3-다차원-배열의-계산"><a href="#3-3-다차원-배열의-계산" class="headerlink" title="3.3 다차원 배열의 계산"></a>3.3 다차원 배열의 계산</h1><h2 id="3-3-1-다차원-배열"><a href="#3-3-1-다차원-배열" class="headerlink" title="3.3.1 다차원 배열"></a>3.3.1 다차원 배열</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(A) <span class="comment"># [1 2 3 4]</span></span><br><span class="line">np.ndim(A) <span class="comment"># 배열의 차원 수 : 1</span></span><br><span class="line">A.shape <span class="comment"># 배열의 형상 : (4,)</span></span><br><span class="line">A.shape[<span class="number">0</span>] <span class="comment"># 4</span></span><br></pre></td></tr></table></figure>
<h2 id="3-3-2-행렬의-내적-행렬-곱"><a href="#3-3-2-행렬의-내적-행렬-곱" class="headerlink" title="3.3.2 행렬의 내적(행렬 곱)"></a>3.3.2 행렬의 내적(행렬 곱)</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.dot(A,B) <span class="comment"># 행렬의 내적</span></span><br></pre></td></tr></table></figure>
<ul>
<li>행렬의 곱에서는 대응하는 차원의 원소 수를 일치시켜라.</li>
</ul>
<h2 id="3-3-3-신경망의-내적"><a href="#3-3-3-신경망의-내적" class="headerlink" title="3.3.3 신경망의 내적"></a>3.3.3 신경망의 내적</h2><p>다차원 배열의 내적을 구해주는 np.dot 함수를 사용해, 단번에 결과를 계산할 수 있다.</p>
<hr>
<h1 id="3-4-3층-신경망-구현하기"><a href="#3-4-3층-신경망-구현하기" class="headerlink" title="3.4 3층 신경망 구현하기"></a>3.4 3층 신경망 구현하기</h1><h2 id="3-4-1-표기법-설명"><a href="#3-4-1-표기법-설명" class="headerlink" title="3.4.1 표기법 설명"></a>3.4.1 표기법 설명</h2><p>이번 절에서만 사용하는 표기이므로 패스</p>
<h2 id="3-4-2-각-층의-신호-전달-구현하기"><a href="#3-4-2-각-층의-신호-전달-구현하기" class="headerlink" title="3.4.2 각 층의 신호 전달 구현하기"></a>3.4.2 각 층의 신호 전달 구현하기</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([<span class="number">1.0</span>, <span class="number">0.5</span>])</span><br><span class="line"></span><br><span class="line">W1 = np.array([<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>], [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.6</span>])</span><br><span class="line">B1 = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">A1 = np.dot(X, W1) + B1 <span class="comment"># 가중치의 합</span></span><br><span class="line">Z1 = sigmoid(A1) <span class="comment"># 활성화 함수</span></span><br><span class="line"></span><br><span class="line">W2 = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>], [<span class="number">0.2</span>, <span class="number">0.5</span>], [<span class="number">0.3</span>, <span class="number">0.6</span>])</span><br><span class="line">B2 = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>])</span><br><span class="line">A2 = np.dot(Z1, W2) + B2</span><br><span class="line">Z2 = sigmoid(A2)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_function</span><span class="params">(x)</span>:</span> <span class="comment"># 항등함수</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">W3 = np.array([<span class="number">0.1</span>, <span class="number">0.3</span>], [<span class="number">0.2</span>, <span class="number">0.4</span>])</span><br><span class="line">B3 = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>])</span><br><span class="line">A3 = np.dot(Z2, W3) + B3</span><br><span class="line">Y = identity_function(A3) <span class="comment"># 혹은 Y = A3</span></span><br></pre></td></tr></table></figure>
<h2 id="3-4-3-구현-정리"><a href="#3-4-3-구현-정리" class="headerlink" title="3.4.3 구현 정리"></a>3.4.3 구현 정리</h2><hr>
<h1 id="3-5-출력층-설계하기"><a href="#3-5-출력층-설계하기" class="headerlink" title="3.5 출력층 설계하기"></a>3.5 출력층 설계하기</h1><p>일반적으로 회귀에는 항등함수, 분류에는 소프트맥스 함수를 사용</p>
<ul>
<li>분류와 회귀 ?<ul>
<li>분류 : 데이터가 어느 클래스에 속하느냐</li>
<li>회귀 : 입력 데이터의 (연속적인) 수치를 예측</li>
</ul>
</li>
</ul>
<h2 id="3-5-1-항등-함수와-소프트맥스-함수-구현하기"><a href="#3-5-1-항등-함수와-소프트맥스-함수-구현하기" class="headerlink" title="3.5.1 항등 함수와 소프트맥스 함수 구현하기"></a>3.5.1 항등 함수와 소프트맥스 함수 구현하기</h2><figure class="highlight py"><figcaption><span>소프트맥스 함수</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(a)</span>:</span></span><br><span class="line">    exp_a = np.exp(a)</span><br><span class="line">    sum_exp_a = np.sum(exp_a)</span><br><span class="line">    y = exp_a / sum_exp_a</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h2 id="3-5-2-소프트맥스-함수-구현-시-주의점"><a href="#3-5-2-소프트맥스-함수-구현-시-주의점" class="headerlink" title="3.5.2 소프트맥스 함수 구현 시 주의점"></a>3.5.2 소프트맥스 함수 구현 시 주의점</h2><p>Sum 부분에 오버플로 문제를 고려해야 한다.</p>
<figure class="highlight py"><figcaption><span>소프트맥스 함수 (overflow 고려)</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(a)</span>:</span></span><br><span class="line">    c = np.max(a)</span><br><span class="line">    exp_a = np.exp(a-c)</span><br><span class="line">    sum_exp_a = np.sum(exp_a)</span><br><span class="line">    y = exp_a / sum_exp_a</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h2 id="3-5-3-소프트맥스-함수의-특징"><a href="#3-5-3-소프트맥스-함수의-특징" class="headerlink" title="3.5.3 소프트맥스 함수의 특징"></a>3.5.3 소프트맥스 함수의 특징</h2><ul>
<li><p>소프트맥스 함수 ?</p>
<blockquote>
<p>입력 받은 값을 0~1 사이의 값으로 모두 정규화 하며 총합은 항상 1이 되는 특성을 가진 함수</p>
</blockquote>
</li>
<li><p>소프트맥스 함수의 출력을 확률로 해석할 수 있다.</p>
</li>
<li>소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다.</li>
</ul>
<h2 id="3-5-4-출력층의-뉴런-수-정하기"><a href="#3-5-4-출력층의-뉴런-수-정하기" class="headerlink" title="3.5.4 출력층의 뉴런 수 정하기"></a>3.5.4 출력층의 뉴런 수 정하기</h2><p>분류에서는 분류하고 싶은 클래스 수로 설정하는것이 일반적</p>
<hr>
<h1 id="3-6-손글씨-숫자-인식"><a href="#3-6-손글씨-숫자-인식" class="headerlink" title="3.6 손글씨 숫자 인식"></a>3.6 손글씨 숫자 인식</h1><h2 id="3-6-1-MNIST-데이터셋"><a href="#3-6-1-MNIST-데이터셋" class="headerlink" title="3.6.1 MNIST 데이터셋"></a>3.6.1 MNIST 데이터셋</h2><p>MNIST : 손글씨 숫자 이미지 집합</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir) <span class="comment"># 부모 디렉터리의 파일을 가져올 수 있도록 설정</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_show</span><span class="params">(img)</span>:</span></span><br><span class="line">    pil_img = Image.fromarray(np.uint8(img))</span><br><span class="line">    pil_img.show()</span><br><span class="line"></span><br><span class="line">(x_train, t_train), (x_test, t_text) = load_mnist(flatten=<span class="keyword">True</span>, normalize=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">img = x_train[<span class="number">0</span>]</span><br><span class="line">label = t_train[<span class="number">0</span>]</span><br><span class="line">print(label)</span><br><span class="line"></span><br><span class="line">print(img.shape)</span><br><span class="line">img = img.reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">print(img.shape)</span><br><span class="line"></span><br><span class="line">img_show(img)</span><br></pre></td></tr></table></figure>
<h2 id="3-6-2-신경망의-추론-처리"><a href="#3-6-2-신경망의-추론-처리" class="headerlink" title="3.6.2 신경망의 추론 처리"></a>3.6.2 신경망의 추론 처리</h2><ul>
<li>이 신경망은 입력층 뉴런을 784(28*28)개, 출력층 뉴런을 10개로 구성</li>
<li>첫번째 은닉층에 50개의 뉴런, 두번째 은닉층에 100개의 뉴런을 배치 (임의로 정한 값)</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">()</span>:</span></span><br><span class="line">    (x_train, t_train), (x_test, t_test) = \</span><br><span class="line">        load_mnist(normalize=<span class="keyword">True</span>, flatten = <span class="keyword">True</span>, one_hot_label=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> x_test, t_test</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_network</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"sample_weight.pkl"</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        network = pickle.load(f)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> network</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(network, x)</span>:</span></span><br><span class="line">    W1, W2, W3 = network[<span class="string">'W1'</span>], network[<span class="string">'W2'</span>], network[<span class="string">'W3'</span>]</span><br><span class="line">    b1, b2, b3 = network[<span class="string">'b1'</span>], network[<span class="string">'b2'</span>], network[<span class="string">'b3'</span>]</span><br><span class="line"></span><br><span class="line">    a1 = np.dot(x, W1) + b1</span><br><span class="line">    z1 = sigmoid(a1)</span><br><span class="line">    a2 = np.dot(x, W2) + b2</span><br><span class="line">    z2 = sigmoid(a2)</span><br><span class="line">    a3 = np.dot(x, W3) + b3</span><br><span class="line">    z3 = sigmoid(a3)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h2 id="3-6-3-배치-처리"><a href="#3-6-3-배치-처리" class="headerlink" title="3.6.3 배치 처리"></a>3.6.3 배치 처리</h2><hr>
<h1 id="참고자료"><a href="#참고자료" class="headerlink" title="참고자료"></a>참고자료</h1><p><a href="https://github.com/Kitchu0401/codenotforfood/blob/master/machine-learning/neural-network.md" target="_blank" rel="noopener">https://github.com/Kitchu0401/codenotforfood/blob/master/machine-learning/neural-network.md</a></p>
</div></article></div></main><footer><div class="paginator"><a class="prev" href="/2018/01/14/DeepLearning/ml001/"><i class="fa fa-arrow-left" aria-hidden="true"></i></a><a class="next" href="/2018/01/28/DeepLearning/ml005/"><i class="fa fa-arrow-right" aria-hidden="true"></i></a></div><ins class="adsbygoogle adsense-bottom" style="display:block" data-ad-client="ca-pub-6188640546219653" data-ad-slot="6129396565" data-ad-format="auto"></ins><div class="fb-comments-area"><div class="fb-comments" data-href="https://ddulhddul.github.io/2018/01/21/DeepLearning/ml002/" data-width="700" data-numposts="5"></div></div><div class="copyright"><p>© 2012 - 2021 <a href="https://github.com/stunstunstun" target="_blank">Minhyeok Jung</a>. Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/stunstunstun/hexo-theme-chiangmai" target="_blank">hexo-theme-chiangmai</a>.</p></div></footer></div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle = window.adsbygoogle || []).push({});</script><script>window.fbAsyncInit=function(){FB.init({appId:"1258629384258634",cookie:!0,xfbml:!0,version:"v2.8"}),FB.AppEvents.logPageView()},function(e,n,t){var o,c=e.getElementsByTagName(n)[0];e.getElementById(t)||((o=e.createElement(n)).id=t,o.src="//connect.facebook.net/en_US/sdk.js",c.parentNode.insertBefore(o,c))}(document,"script","facebook-jssdk");</script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create','UA-97419941-1','auto');ga('send','pageview');</script></body></html>