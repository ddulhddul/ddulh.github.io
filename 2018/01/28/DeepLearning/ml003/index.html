<!DOCTYPE html><html prefix="og: http://ogp.me/ns#"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title>Chapter4. 신경망 학습 · ddulh's</title><meta name="description" content="잡념"><meta name="og:title" content="Chapter4. 신경망 학습"><meta name="og:type" content="website"><meta name="og:url" content="https://ddulhddul.github.io/2018/01/28/DeepLearning/ml003/"><meta name="og:image" content="http://image.toast.com/aaaaahq/hola_cover.JPG"><meta name="og:description" content="잡념"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/chiangmai.css"><meta name="steem:author" content="@stunstunstun"><link rel="search" type="application/opensearchdescription+xml" href="https://ddulhddul.github.io/atom.xml" title="ddulh's"></head><body class="post"><div id="fb-root"></div><div class="wrap"><header><nav class="navi-post"><a class="navi-post-back" href="javascript:history.back()"><i class="fa fa-arrow-left" aria-hidden="true"></i></a><a class="navi-post-home" href="/"><i class="fa fa-home" aria-hidden="true"></i></a></nav></header><main class="post"><div class="post"><article class="post-block"><h1 class="post-title">Chapter4. 신경망 학습</h1><div class="post-info"><div class="post-info-details"><div class="post-categories"><a href="/categories/DeepLearning" target="_self"><span>DEEPLEARNING</span></a></div><div class="post-date">Jan 28, 2018</div></div></div><div class="post-share"><div class="fb-like" data-href="https://ddulhddul.github.io/2018/01/28/DeepLearning/ml003/" data-layout="button_count" data-action="like" data-size="small" data-show-faces="true" data-share="false">                 </div><div class="fb-share-button" data-href="https://ddulhddul.github.io/2018/01/28/DeepLearning/ml003/" data-layout="button" data-size="small" data-mobile-iframe="true"></div></div><div class="post-content"><p><img src="/images/deeplearning/cover.jpg" alt="밑바닥부터 시작하는 딥러닝"></p>
<h1 id="4-1-데이터에서-학습한다"><a href="#4-1-데이터에서-학습한다" class="headerlink" title="4.1 데이터에서 학습한다!"></a>4.1 데이터에서 학습한다!</h1><ul>
<li>가중치 매개변수의 값을 데이터를 보고 자동으로 결정</li>
</ul>
<h2 id="4-1-1-데이터-주도-학습"><a href="#4-1-1-데이터-주도-학습" class="headerlink" title="4.1.1 데이터 주도 학습"></a>4.1.1 데이터 주도 학습</h2><ul>
<li>신경망돠 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지님.</li>
<li>이미지에서 특징을 추출하고 특징의 패턴을 기계학습 기술로 학습</li>
<li>이미지의 특징은 보통 벡터로 기술</li>
<li>다만, 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 사람이 설계</li>
</ul>
<p><img src="/images/deeplearning/chapter4.01paradigm.PNG" alt="Paradigm"></p>
<blockquote>
<p>회색은 사람이 개입하지 않음.<br>딥러닝을 종단간 기계학습 이라고도 한다. (처음부터 끝까지)</p>
</blockquote>
<h2 id="4-1-2-훈련-데이터와-시험-데이터"><a href="#4-1-2-훈련-데이터와-시험-데이터" class="headerlink" title="4.1.2 훈련 데이터와 시험 데이터"></a>4.1.2 훈련 데이터와 시험 데이터</h2><ul>
<li>기계학습의 문제는 훈련 데이터와 시험 데이터를 나눠 실험을 수행하는 것.<ul>
<li>범용적으로 사용할 수 있는 모델을 원하기 때문.</li>
<li>범용 능력을 제대로 평가하기 위해 훈련 데이터와 시험 데이터를 분리.</li>
</ul>
</li>
<li>데이터셋 하나에만 지나치게 최적화되어 오버피팅 된 상태가 벌어지기도 한다.</li>
</ul>
<h1 id="4-2-손실-함수"><a href="#4-2-손실-함수" class="headerlink" title="4.2 손실 함수"></a>4.2 손실 함수</h1><ul>
<li>신경망 학습에서 현재의 상태를 하나의 지표로 표현한다.</li>
<li>신경망 학습에서 사용하는 지표는 <strong>손실 함수</strong> (loss function) 라고 한다.<ul>
<li>일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용.</li>
</ul>
</li>
</ul>
<blockquote>
<p>손실함수는 신경망의 성능의 나쁨을 나타내는 지표이다. 손실 함수에 마이너스를 곱하면 얼마나 좋으냐 라는 지표로 변한다. 성능의 나쁨과 좋음중 어느쪽을 지표로 삼아도 본질적으로 같다.</p>
</blockquote>
<h2 id="4-2-1-평균-제곱-오차"><a href="#4-2-1-평균-제곱-오차" class="headerlink" title="4.2.1 평균 제곱 오차"></a>4.2.1 평균 제곱 오차</h2><p>mean squared error, MSE</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e" alt="MSE 함수"></p>
<blockquote>
<p>Yi : 신경망의 출력(신경망이 추정한 값)<br>Y^i : 정답 레이블<br>i : 데이터의 차원 수</p>
</blockquote>
<p>출처 : <a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank" rel="noopener">위키백과</a></p>
<p>책에 나온 MSE 함수와 위키에 나온 함수가 다르다 ?</p>
<ul>
<li>평균 제곱 오차를 기준으로 오차가 더 작은 값이 정답에 더 가까운 것.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_squared_error</span><span class="params">(y,t)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.sum((y-t)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="4-2-2-교차-엔트로피-오차"><a href="#4-2-2-교차-엔트로피-오차" class="headerlink" title="4.2.2 교차 엔트로피 오차"></a>4.2.2 교차 엔트로피 오차</h2><p>cross entropy error, CEE</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293" alt="CEE 함수"></p>
<blockquote>
<p>q(x) : 신경망의 출력<br>p(x) : 정답 레이블</p>
</blockquote>
<p>출처 : <a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noopener">위키백과</a></p>
<ul>
<li>실질적으로 정답일때의 추정의 자연로그를 계산하는 식이 된다.</li>
<li>교차엔트로피 오차는 정답일 때의 출력이 전체 값을 정한다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span><span class="params">(y,t)</span>:</span></span><br><span class="line">    delta = <span class="number">1e-7</span></span><br><span class="line">    <span class="keyword">return</span> -np.sum(t * np.log(y+delta))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>아주작은 값인 delta 를 더하는 이유는 np.log 에 0을 입력하면 마이너스 무한대로 가기때문.</p>
</blockquote>
<h2 id="4-2-3-미니배치-학습"><a href="#4-2-3-미니배치-학습" class="headerlink" title="4.2.3 미니배치 학습"></a>4.2.3 미니배치 학습</h2><p>기계학습은 훈련데이터를 사용해 학습을 해야하는것. 훈련데이터 모두에 대한 손실 함수의 합을 구하자</p>
<p>빅 데이터 수준이 되면, 손실 함수의 합을 구하는데 시간이 걸린다. 따라서 근사치를 이용. (미니배치)</p>
<ul>
<li>미니배치 예 (MNIST)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys, os</span><br><span class="line">sys.path.append(os.pardir)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> dataset.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"></span><br><span class="line"><span class="comment"># batch size 만큼 랜덤 추출</span></span><br><span class="line">train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">batch_mask = np.random.choice(train_size, batch_size)</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; np.random.choice(60000, 8)</span></span><br><span class="line"><span class="comment"># array([8013, 14666, 5777, 12351, 12352, 1235, 3463 ,347])</span></span><br><span class="line"></span><br><span class="line">x_batch = x_train[batch_mask]</span><br><span class="line">t_batch = t_train[batch_mask]</span><br><span class="line"></span><br><span class="line">(x_batch, t_batch), (x_test, t_text) = load_mnist(normalize=<span class="keyword">True</span>, one_hot_label=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">print(x_train.shape) <span class="comment"># (60000, 784)</span></span><br><span class="line">print(t_train.shape) <span class="comment"># (60000, 10)</span></span><br></pre></td></tr></table></figure>
<h2 id="4-2-4-배치용-교차-엔트로피-오차-구현하기"><a href="#4-2-4-배치용-교차-엔트로피-오차-구현하기" class="headerlink" title="4.2.4 (배치용) 교차 엔트로피 오차 구현하기"></a>4.2.4 (배치용) 교차 엔트로피 오차 구현하기</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span><span class="params">(y,t)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> y.dim == <span class="number">1</span>:</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line"></span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># one_hot_label</span></span><br><span class="line">    <span class="keyword">return</span> -np.sum(t * np.log(y)) / batch_size</span><br><span class="line">    <span class="comment"># else</span></span><br><span class="line">    <span class="keyword">return</span> -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size</span><br></pre></td></tr></table></figure>
<h2 id="4-2-5-왜-손실-함수를-설정하는가"><a href="#4-2-5-왜-손실-함수를-설정하는가" class="headerlink" title="4.2.5 왜 손실 함수를 설정하는가 ?"></a>4.2.5 왜 손실 함수를 설정하는가 ?</h2><p>신경망 학습에서는 최적의 매개변수(가중치, 편향)를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다. 이때 매개변수의 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다.</p>
<p>가중치 매개변수의 손실 함수의 미분이란, ‘가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하냐’</p>
<blockquote>
<p>신경망을 학습할 때 정확도를 지표로 삼아서는 안된다. 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.</p>
</blockquote>
<p>신경망 학습에서 중요한 성질로, 기울기가 0이 되지 않는것. (시그모이드 함수)</p>
<h1 id="4-3-수치-미분"><a href="#4-3-수치-미분" class="headerlink" title="4.3 수치 미분"></a>4.3 수치 미분</h1><h2 id="4-3-1-미분"><a href="#4-3-1-미분" class="headerlink" title="4.3.1 미분"></a>4.3.1 미분</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 나쁜 구현의 예</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numerical_diff</span><span class="params">(f,x)</span>:</span></span><br><span class="line">    h = <span class="number">10e-50</span></span><br><span class="line">    <span class="keyword">return</span> (f(x+h) -f(x)) /h</span><br></pre></td></tr></table></figure>
<ul>
<li>h에 가능한 작은 값을 사용했으므로, 반올림 오차 문제를 야기한다.</li>
<li>함수의 차분에 대한 문제. 무한히 0으로 좁히는데는 한계가 있다.</li>
<li>전후의 차분을 계산하는 중심차분으로의 구현</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numerical_diff</span><span class="params">(f,x)</span>:</span></span><br><span class="line">    h = <span class="number">1e-4</span> <span class="comment"># 0.0001</span></span><br><span class="line">    <span class="keyword">return</span> (f(x+h) -f(x-h)) / (<span class="number">2</span>*h)</span><br></pre></td></tr></table></figure>
<h2 id="4-3-2-수치-미분의-예"><a href="#4-3-2-수치-미분의-예" class="headerlink" title="4.3.2 수치 미분의 예"></a>4.3.2 수치 미분의 예</h2><h2 id="4-3-3-편미분"><a href="#4-3-3-편미분" class="headerlink" title="4.3.3 편미분"></a>4.3.3 편미분</h2><p>변수가 여럿인 함수에 대한 미분</p>
<p>변수중 하나의 변수에 초점을 맞추고 다른 변수의 값은 고정하여 편미분의 값을 구할 수 있다.</p>
<h1 id="4-4-기울기"><a href="#4-4-기울기" class="headerlink" title="4.4 기울기"></a>4.4 기울기</h1><p>모든 변수의 편미분을 벡터로 정리한 것을 <strong>기울기</strong> 라고 한다.</p>
<h1 id="작성중"><a href="#작성중" class="headerlink" title="작성중"></a>작성중</h1></div></article></div></main><footer><div class="paginator"><a class="prev" href="/2018/01/28/DeepLearning/ml005/"><i class="fa fa-arrow-left" aria-hidden="true"></i></a><a class="next" href="/2018/03/02/DeepLearning/ml009/"><i class="fa fa-arrow-right" aria-hidden="true"></i></a></div><div class="copyright"><p>© 2012 - 2021 <a href="https://github.com/stunstunstun" target="_blank">Minhyeok Jung</a>. Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/stunstunstun/hexo-theme-chiangmai" target="_blank">hexo-theme-chiangmai</a>.</p></div></footer></div></body></html>